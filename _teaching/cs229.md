---
title: "Toxic Comment Detection and Classification"
excerpt: 'Preprocess dataset after word cloud and time series analysis, and apply weighted-loss to solve data imbalance problem. Implement Naïve Bayes-Support Vector Machines, LSTM and BERT models, and apply two ensemble methods to improve quantitative performance (84.28\%F1, 95.14\%EM).'
collection: projects
date: 2019-1-7
permalink: /teaching/cs229
location: "City, Country"
venue: "Stanford University, Computer Science Department"
type: "Academic project"
---


# Toxic Comment Detection and Classification
---

Introduction
============

Internet is an open discussing space for everyone to freely express
their opinions. However, harassment and abuse are discouraging people
from sharing their ideas and disturbing the internet environment.
Platforms struggle to effectively facilitate conversations, leading many
communities to limit or completely shut down user comments if it’s
toxic.

Motivated by this problem, we want to build technology to protect voices
in conversation by machine learning models that can identify toxicity in
online conversations, where toxicity is defined as anything rude,
disrespectful or otherwise likely to make someone leave a discussion.

For our model, the input will be a comment. We used three different
models to predict scores for “toxicity" (which is our target),
“severe\_toxicity", “obscenity", “identity\_attack", “insult", “threat".
Then the ensemble methods are studied to combine multiple learning
algorithms and get the best result.

Related Work
============

Related research has looked into hate speech, online harassment, abusive
language, cyberbullying, and offensive language. Generally speaking,
toxic comment detection is a supervised classification task and can be
approached by either manual feature engineering[@relatedWork1] or neural
networks[@relatedWork2].

A large variety of machine learning approaches have been explored to
tackle the detection of toxic language. Fahim Mohammad[@relatedWork]
compared traditional machine learning methods, from Naive Bayes (NB) and
logistic regression (LR) to state-of-art neural networks. The
performances of BiLSTM and NB-SVM are good and robust after data
preprocessing compared to other models. Zimmerman et al[@relatedWork5]
investigated ensemble models with different hyper-parameters, which
inspired us to combine various model architectures and different word
embeddings for toxic comment classification.

Neural network approaches appear to be more effective[@relatedWork3],
while feature-based approaches preserve some sort of explainability. In
this paper we used NB-SVM as our baseline model, and later switched to
explore deep neural networks (e.g. BiLSTM) and a new language
representation model Bert[@relatedWork4], which stands for bidirectional
encoder representations from transformers. Bert is designed to pretrain
deep bidirectional representations from unlabeled text by jointly
conditioning on both left and right contexts in all layers.

Dataset and Features
====================

Data description
----------------

To train our models, we use the Civil Comments dataset from
Kaggle.[@dataset] The dataset comprises of over 1804000 rows. Each row
contains a general toxic target score from 0 to 1, a comment text,
scores under various labels such as severe toxicity, obscene, identity
attack, insult, threat, asian, homosexual gay or lesbian, black,
intellectual or learning disability, etc. and other information
including the article id and rating, likes, disagrees of the comment and
so on. The data distribution is shown in figure\[datar\], which is quite
imbalanced for training. Some efforts to fix this problem will be
discussed later.

![Data
distribution[]{data-label="datar"}](fig/datarate.png){width="0.95\linewidth"}

The dataset is split into 80% as training set, 10% as dev set and 10% as
test set.

Time series analysis
--------------------

In order to have a better understanding of the data distribution, we
first checked the time series for toxicity regarding to different
identities. We mainly investigated four toxicity sub-type attributes.

[0.5]{} ![time series
analysis[]{data-label="fig:time"}](fig/time_race.png "fig:"){width=".9\linewidth"}

[.5]{} ![time series
analysis[]{data-label="fig:time"}](fig/time_sexual.png "fig:"){width=".9\linewidth"}

[.5]{} ![time series
analysis[]{data-label="fig:time"}](fig/time_religion.png "fig:"){width=".9\linewidth"}

[.5]{} ![time series
analysis[]{data-label="fig:time"}](fig/time_disab.png "fig:"){width=".9\linewidth"}

As shown in figure \[fig:time\], we can see that the data was collected
between 2016 and 2017. And there was a interesting peak around Jan 2017.
To better understand the underlying reason, we found out that the top
three common words appear in the toxic comment are “trump", “people"
and“like", which explains why there is a peak in Jan 2017.

Data preprocess
---------------

Based on the analysis, we did the following data preprocess:

1.  Besides the common word contraction mappings, like “it’d“ to ”it
    would", we added word contraction mappings that were related to our
    dataset, like “Trump’s" to “Trump is", “Trumpcare" to “Trump care",
    “Obamacare" to “Obama care" and so on.

2.  We collected some common misspell words and corrected them, such as
    “tRump" to “Trump".

3.  We translated some special Latin words and Emojis to English words.

Methods
=======

Naive Bayes SVM Model
---------------------

We combined Naive Bayes and Support Vector Machines to serve as our
baseline model. SVM is built over NB log-count ratios in feature values,
and it has been proven a strong and robust performer over all the
presented tasks.

First, we will introduce the notation here. $f^{i}\in \mathcal{R}^{|V|}$
is the word count vector for comment $i$ with label $y^{i}\in \{-1,1\}$
indicating whether it’s toxic or not. V is the dictionary, the set of
all words. $f^{i}_{j}$ is the number of occurrences of word $V_j$ in
comment i. The log-count ratio vector is $r\in \mathcal{R}^{|V|}$.

$$r_{j} = log(\frac{1+\sum_{i:y^{i}=1}{f^i_{j}}}{1+\sum_{i:y^{i}=-1}{f^i_{j}}})$$

We formulated our model as a linear classifier, where the prediction for
comment k in the test set is $y^k$.

$$y^k=sign(w^Tx^k+b)$$

For multinomial event Naive Bayes model,
$$x^k=f^k, w=r, b=log(\frac{N_+}{N_-})$$ where $N_+, N_-$ are the number
of toxic and nontoxic comments in the training set.

For the SVM model with Naive Bayes log-count ratios in feature values,
$$x^k=r \circ {f}^k\textrm{(element-wise product)}$$ $w$ and $b$ are
obtained by solving the optimization problem:
$$\min_{w,b}\ \frac{1}{2}w^Tw$$
$$\textrm{s.t.}\ -y^i({w^T}{x^i}+b))+1\leq0,\ i=1,...,n$$

LSTM model
----------

Long short-term memory networks (LSTMs) are a special kind of RNNs
designed to be capable of learning long-term dependencies. Vanilla RNNs
can be tough to train on long sequences due to vanishing and exploding
gradients caused by repeated matrix multiplication. LSTMs solve this
problem by introducing a hidden cell and replacing the simple update
rule of the vanilla RNN with a gating mechanism.

![LSTM structure](fig/LSTM.png){width="0.95\linewidth"}

Frequently, the dependencies within sequential data, like sentences, are
not just in one direction, but may be observed in both directions.
Therefore, we used BiLSTMs, in which, Bidirectional layers exploit the
forward and backward dependencies by combining the features obtained
going in both directions simultaneously.

![BiLSTM structure](fig/bi-LSTM.png){width="0.95\linewidth"}

We first converted every word in the input text to a vector based on
word embeddings. Then we dropped out vectors at random positions to
increase the robustness of our model. At the end of our model, in
addition to the target score of toxicity, the model also predicted an
auxiliary result, which not only included the score for toxicity, but
also the scores for more specific characteristics of the comment text,
such as “obscenity", “identity attack", “insult" and “threat". These
scores are provided in the training set. Though these are not the target
score we need from the model, the model predicted them to utilize these
additional information in data.

![LSTM model](fig/model.png){width="0.95\linewidth"}

BERT model
----------

Bidirectional Encoder Representations from Transformers(BERT) achieves
state-of-the-art performance for eleven NLP tasks, including Question
Answering and Sentence (and sentence-pair) classification tasks, through
only fine-tuning the last layer. BERT has such noteworthy achievement
because it learns a more powerful bi-directional representation than
most of the previous approaches. BERT’s architecture is mainly
multi-layer bidirectional Transformer encoder with bidirectional
self-attention mechanism. The encoder of BERT is pre-trained with two
tasks, “masked language model” (MLM) and Next Sentence Prediction. These
two objectives help encoder to learn both left and right contextual of a
word in the sentence and provides significant support for downstream
tasks like question answering.

We used the pre-trained BERT-Base model, which is cased and has 12 layer
with 768-hidden, 12-heads, and 110M total parameters. Cased means that
the true case and accent markers are preserved. To integrate the
pre-trained encoder of BERT and fine-tune it to solve sentence
classification problem, a final classification layer is added with
weights $W \in R^{H \times K}$, where H is the encoded hidden size and K
is the number of classifier labels. The label probabilities $P \in R^K$
are computed with a softmax layer, $P = \text{softmax}(CW^T)$.

Ensembling
----------

In modern Machine Learning, Ensembling Methods are extensively used to
combine multiple learning algorithms, preferably from different model
classes, into an aggregate model with better performance than any single
model. Each of the BERT and LSTM model we built make different
predictions on probabilities of toxicity, and therefore we use
Ensembling Methods in hope of utilizing the information extracted from
all models. Specifically, we create an algorithm in combining these
models.

**Input:**\
1. set of k models: $M \in R^k$\
2. dev set:
$\{(P_l = (probability of toxicity)\}_{l\in 1, 2... n_{dev}}$\
3. prediction set: $\{P_m\}_{m\in 1, 2... n_{pred}}$\
4. max\_num\_iter\
**Output:** Predictions on the prediction set\
$best\_weight = Guided\_Random\_Search\_on\_Weight$(M, max\_num\_iter,
dev set)\
**Return** $Make\_Predictions$(M, $best\_weight$, prediction set)\

Assume that we have a total of n models to ensemble, and for each
inputting (Question, Paragraph) pair, model k outputs $$\begin{aligned}
    ^{k}p_{i},  0\leq i \leq \text{len(dev examples)}\end{aligned}$$
where $^{k}p_{i}$ is the predicted probability of the i-th dev example
of the k-th model. Following this notation, our goal is now reduced to
finding the best $w\in R^n$, such that $$\begin{aligned}
    p_{i} &:= \sum_{k}w_k\times p^{k}_{i}, w \in R^n\\\end{aligned}$$
reaches the most accurate prediction. With this motivation, we develop a
pipeline(figure\[pipeline\]) that learns the weights $w$, and make
predictions on the dev set. The details are described in
Algorithm\[Ensembling\_weight\_alg\]. In high level, our algorithm
randomly assign weights to each model. With the weights learned, we
re-do the predictions by taking the weighted average of the
probabilities predicted by each model.

![Pipeline for Guided Random Search + Weighted Average
Ensembling[]{data-label="weighted_ensembling"}](fig/pipe.png){width="0.9\columnwidth"}

\[pipeline\]

Another ensembling algorithm we would like to explore is most-confident
voting. The idea is inspired from the data distribution. As we can see
that there is more nontoxic comments than toxic comments in training
set. The model tends to predict nontoxicity for many comments. Thus we
would like to believe that as long as one of those model conforms a
comment toxicity, it is more likely that it is the true ground.

Evaluation Method
=================

We mainly used two types of evaluation metrics, Exact Match and F1
score. Exact Match (EM) is the percentage of outputs that match exactly
the ground truth. F1 is the harmonic mean of precision and recall:
$$F_1 = \frac{2\times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$$
$$\text{precision} = \frac{\text{true positives}}{\text{true positives} + \text{false positives}}$$
$$\text{recall} = \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}$$
where true positives are the toxic comments predicted as toxic, false
positives are the nontoxic comments predicted as toxic, false negative
are the toxic comments predicted as toxic.

Experiments and Results
=======================

Naive Bayes SVM Model
---------------------

The evaluation of our baseline model, Naive Bayes SVM, is shown in Table
\[NB\]. Naive Bayes SVM Model reached a high EM score. However, the F1
score was lower than EM by around 20%. It resulted from our model’
tendency to predict comments as nontoxic. This can be explained by the
imbalance of data, which had a much larger percentage of nontoxic
comments than toxic comments.

  Description   Dev F1   Dev EM
  ------------- -------- --------
  Naive Bayes   68.33%   87.57%

  : Naive Bayes Result[]{data-label="NB"}

LSTM Model
----------

We trained the LSTM model for 4 epochs using the Adam optimizer with the
initial learning rate 1e-3. We set a learning rate scheduler adjusting
the learning rate at the end of every epoch during training. We used
binary cross entropy loss as the loss function.

Table \[weighted loss acc\] shows the evaluation of the LSTM model. From
the result, we can see that both EM score and F1 score are much higher
than Naive Bayes SVM model. It is caused by the Naive Bayes model’s
ignorance of the relationship between words, which is a fatal problem.
LSTM could remember the relationships and combine those meaning together
to get the result, and it led to a more accurate result than our
baseline model.

Weighted Loss LSTM
------------------

There are several methods put forward to solve the imbalanced data
problem. We tried weighted loss to train our model. We set the weight of
the loss of predictions for toxic comments, larger than the weight of
the loss of predictions for nontoxic comments. From figure\[datar\], we
can see that the rate of nontoxic comments and toxic comments is about
9:1. Thus we set the weights to be 0.9 for toxic comments and 0.1 for
nontoxic comments. The results are shown in the Figure \[weighted loss
acc\]. The results indicated that weighted loss can be a very useful
hyperparameter to fine-tune model trained on imbalanced data.

Data Preprocess with Contraction Mapping
----------------------------------------

Based on LSTM model discussed above, we tried to find whether the data
preprocess with “Trump" words will work. We set weights of loss 0.5 and
0.5. We trained one LSTM model with contraction mapping and one without.
The results are shown in table \[datapre\].

  Description                   Dev F1   Dev EM
  ----------------------------- -------- --------
  With contraction mapping      77.95%   95.37%
  Without contraction mapping   76.04%   95.38%

  : Comparison between data preprocess[]{data-label="datapre"}

The results quite make sense. As mentioned before, the most frequent
word in toxic comments is “Trump". Thus, if we preprocess those “Trump"
words, the toxicity in the training set will be more obvious. The result
reflects the effectiveness of contraction mapping. The improvement of F1
score shows that the model became better at detecting toxic comments.

BERT
----

Because of time limit, we trained BERT model with only one epoch and got
results comparable to the LSTM model. Besides, batch size was set to be
32 due to the limitation of GPU memory. Learning rate was set to be
$2\times 10^{-2}$, which is recommended by the official guideline.

With the same setting, we also trained the BERT model using weighted
loss (0.9 for toxic comments and 0.1 for nontoxic comments). The results
are shown in table \[bert\].

Ensemble Method
---------------

With the above models, we ran our ensemble algorithm and the results are
listed in table \[ensemble\].

The first method we used was most-confident voting, which takes the most
confident result with the highest confidence. This could work because
that the prediction mistakes mainly occurred on toxic comments and the
F1 score was relatively lower than the EM score. So if we took the most
confident result, the toxic comments would be more likely to be
detected. As shown in the table, the F1 score increases without the
compromise of decreasing EM score. The results were gotten from BERT
with weighted loss, BERT without weighted loss and LSTM with weighted
loss.

The second method is guided-random-research with weighted average
ensembling. The results were gotten from Bert with weighted loss and
LSTM with weighted loss.

  Description                           Dev F1   Dev EM
  ------------------------------------- -------- --------
  Best model without ensembleing        81.19%   95.54%
  Ensembling with most confident vote   84.28%   95.14%
  Ensembling with guided weight         81.57%   95.50%

  : BERT model and ensembling[]{data-label="ensemble"}

Conclusion
==========

To sum up our work, we implemented three machine learning models, namely
Naive Bayes-SVM model, LSTM and BERT. We used information from data
visualization to preprocess data. Weighted loss performed very well in
fixing the problem of imbalanced data. BERT can produce state-of-the-art
work with only one output layer. We trained BERT with only one epoch and
got results better than the LSTM model. At last, we used two ensemble
methods to further improve the quantitative results.

For future work, we would like to try some other ways besides weighted
loss to fix the problem of imbalanced data. BERT model brought us
surprisingly good results and we would like to explore it if we have
more time.

Acknowledgements
================

We would like to acknowledge the inspiration of the Naive Bayes SVM
model and the base LSTM network model which provided us the starting
point of our code:

https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline

https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version
