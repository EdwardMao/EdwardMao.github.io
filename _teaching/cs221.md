---
title: "Deep Reinforcement Learning and Its Regularization in openAI Gym"
excerpt: 'Implement simple reinforcement learning algorithm Q-learning and deep reinforcement learning algorithm Policy Gradient to teach agents solve control tasks in different environment of openAI gym. Implement variants of policy gradient such as proximal policy optimization and research into the effects of different regularization methods on policy gradient algorithm.'
collection: projects
date: 2018-9-1
permalink: /teaching/cs221
location: "City, Country"
venue: "Stanford University, Computer Science Department"
type: "Academic project"
---

# Improving Reinforcement Learning Algorithms: Deep Reinforcement Learning and its Regularization

Introduction
============

In the past five years, reinforcement learning has attracted increasing
interests in Artificial Intelligence communities. It is especially
useful in that it directs agents-planning by rewards and punishments,
and thus overrides the complexity of studying on the underlying
mechanisms and inherent physics dynamics of the tasks. In this paper, we
will experiment on the effects of Reinforcement Learning in teaching
agents to solve control tasks in the environments in openAI Gym
(<https://gym.openai.com>). The environments we are planning to
investigate on include CartPole-v0, InvertedDoublePendulum-v1 and
HalfCheetah-v2. Specifically, We will compare simple Reinforcement
Learning algorithms such as Q-Learning with deep Reinforcement Learning
algorithms such as Policy Gradients. After that, we will implement
variants of Policy Gradients such as PPO, and research into the effects
of different regularization methods on Policy Gradient Algorithm, which
has not been proved to provide benefits for reinforcement learning. From
these case studies, we compare their performances in different
environments mentioned above and seek for improvements.

Literature Review
=================

The Arcade Learning Environment (ALE)[@bellemare2013arcade] is a popular
benchmark for evaluating algorithms designed for tasks with
high-dimensional state inputs and discrete actions. However, in the
continuous control domain, where actions are continuous and often
high-dimensional, these benchmark fails to provide a comprehensive set
of challenging problems. For example, algorithms based on Q-learning
quickly become infeasible when naive discretization of the action space
is performed, due to the curse of dimensionality. In Duan’s
paper[@duan2016benchmarking], benchmarking Deep Reinforcement Learning
for Continuous Control, a benchmark of continuous control problems for
reinforcement learning is implemented, including cart-pole balancing
which will be covered later. And a range of reinforcement learning
algorithms are implemented in this paper such as policy gradient
methods. However, policy gradient methods have poor data efficiency and
robustness. John[@PPO] developed a method called proximal policy
optimization (PPO) that is scalable (to large models and parallel
implementations), data efficient, and robust (successful on a variety of
problems without hyperparameter tuning). We will implement this
algorithm later.

Task Definition
===============

OpenAI Gym is a toolkit for developing and comparing reinforcement
learning algorithms, and each environment is a specific task for
learning agents. By providing an environment name to the Gym package,
Gym is able to return an observation of the environment, which is a
partially observed state for the problem. Gym also provides us with the
method to get all the possible actions, as well as the rewards for
observation-action pairs. We could then use the observations and actions
as inputs to our reinforcement learning algorithms, and get the reward
as the outputs. The environments we use for this project include
CartPole-v0, InvertedDoublePendulum-v1 and HalfCheetah-v2:

CartPole-v0
-----------

*Figure \[cartpole\]* displays a CartPole-v0 game environment with the
goal to balance a pole on a cart and prevent falling as long as
possible. To perform reinforcement learning, the inputs to CartPole-v0
is defined as:

![CartPole-v0
Example[]{data-label="cartpole"}](CartPole.png){width="0.5\columnwidth"}

-   State: 4 continuous state variables: Cart Velocity, Pole Angle, Top
    Tip Pole Velocity and Bottom Tip Pole Velocity.

-   Action: 2 discrete actions for every state, pushing cart to left(0)
    or pulling to right(1)

-   Reward: Therefore, reward is 1 for every step taken, including the
    termination step.

-   Starting state: All state variables starting with a uniform random
    value between $\pm$ 0.05.

-   Termination state: Pole Angle is more than $\pm0$ 12 degree; Cart
    Position is more than $\pm0$ 2.4; Time episode length is greater
    than 200.

-   Transition probability: Deterministic based on the action.

InvertedDoublePendulum-v1
-------------------------

*Figure \[invertedDoublePendulum\]* displays a InvertedDoublePendulum-v1
game environment. InvertedDoublePendulum is constructed by an inverted
pendulum and a double pendulum. The goal is also to balance it and
prevent it moving. There are two main approaches to control it, moving
the base on the inverted pendulum, or applying a torque on the pivot
point between the two pendulums. More specifically The inputs to
InvertedDoublePendulum-v1 is defined as:

![InvertedDoublePendulum-v1
Example[]{data-label="invertedDoublePendulum"}](InvertedDoublePendulum.png){width="0.5\columnwidth"}

-   State: 11 continuous state variables including: linear position, sin
    and cos of 2 joint angles, velocities of position & 2 joint angles,
    constraint forces on position & 2 joint angles.

-   Action: Continuous variable representing the applied joint effort

-   Reward: Bonus of alive minus the penalty of distance and velocity

-   Starting state: Position start with a uniform random value between
    $\pm$ 0.1. Initial velocity samples from Normal(0, 0.1)
    distribution. Random angle from $-\pi$ to $\pi$.

-   Termination state: Time episode length is greater than 200.

-   Transition probability: Deterministic based on the action.

HalfCheetah-v2
--------------

*Figure \[halfcheeta\]* displays a HalfCheetah-v2 game environment,
which simulates a cheetah with only two legs running forward. The
cheetah’s goal is to maximize its speed moving forward.

![HalfCheetah-v2
Example[]{data-label="halfcheeta"}](halfcheeta.jpeg){width="0.5\columnwidth"}

-   State: 18 continuous variables including the position, velocity,
    angle, and angle velocity at each joint of the cheetah.

-   Action: 6 continuous action variables as the torque on each hinge.

-   Reward: Cheetah gets reward to run in forward direction.

-   Starting state: Position start with a uniform random value between
    $\pm$ 0.1. Initial velocity samples from Normal(0, 0.1)
    distribution.

-   Termination state: Time episode length is greater than 200.

-   Transition probability: Deterministic based on the action.

Approach
========

Q-Learning
----------

0.2in

![Q-Learning
Algorithm[]{data-label="qlearning"}](qlearning.png){width="\columnwidth"}

-0.2in

Q-learning is a form of model-free reinforcement learning. It provides
agents with the capability of learning to act optimally in Markovian
domains by experiencing the consequences of actions. Starting from the
current state, Q-learning finds an optimal policy that maximizes the
expected value of the total reward over all successive steps. By trying
all actions in all states repeatedly, it learns which is best overall,
judged by long-term discounted reward.

Following the diagram in *Figure \[qlearning\]*, the details of the
implementation is depicted in *Algorithm \[QL\]*:

**Repeat** until a terminal state is reached: Initialize the Q-value
randomly Observe the current state Choose an action for the state based
on the maximum Q-value Step the action, observe the reward and the new
state Update the Q-value for the state-action pair using the observed
reward and maximum reward possible Set the state to the new state

Formally, on each ($s,a,r,s^{'}$),
$$\hat{Q}_{opt}(s,a)\xleftarrow{}(1-\eta)\hat Q_{opt}(s,a)+\eta(r+\gamma\underset{a^{'}\in A(s^{'})}{max}\hat Q_{opt}(s^{'} ,a^{'} ))$$

Vanilla Policy Gradients [@PG]
------------------------------

0.2in

![Policy Gradients
Algorithm[]{data-label="policy_gradients"}](policy_gradients.png){width="\columnwidth"}

-0.2in

Unlike Q-learning, which is a value based reinforcement learning
algorithm that gives each $(s, a)$ pair an estimated Q-value and chooses
the action that maximizes the Q-value for each state, Policy Gradient
learns the policy directly by optimizing a policy function
($\pi_{\theta}$) that maps from states to actions.

In addition, Policy Gradients is a deep Reinforcement Learning algorithm
that uses deep neural networks to approximate the policy function
$\pi_{\theta}$. The implication is that Policy Gradients has a strong
representation power because by changing the number of layers, hidden
sizes, activation functions, and other architecture of neural networks,
$\pi_{\theta}$ can represent any function class.

Following the diagram in *\[policy\_gradients\]*, the specific algorithm
for Policy Gradients is shown in *Algorithm \[PG\]*.

**for** t in 1, ... T or until convergence: Run the policy to sample
${\tau^i}$ from $\pi_{\theta}(a_t|s_t)$
$\nabla_{\theta}J(\theta) \approx \sum_i(\sum_t\nabla_{\theta}\log\pi_{\theta}(a_t^i|s_t^i))(\sum_t r(s_t^i, a_t^i))$
$\theta \leftarrow \theta + \alpha\nabla_{\theta}J(\theta)$ **end for**

There are several key components in applying Policy Gradients:

**Policy Score Function:** Define the learned policy $\pi_{\theta}(a|s)$
as the probability to take action a, for a given sate s. Then define
policy score function as $J(\theta) = E_{\pi_{\theta}(\tau)}[r(\tau)]$,
where $\tau = (s_1, a_1, s_2, a_2, \dots, s_n, a_n)$ is a random
variable of sequence of (state, action) pairs following the stochastic
policy defined by $\pi_{\theta}$, and $r(\tau)$ is the discounted reward
of this sequence of (state, action) pairs. With mathematical derivation,
the gradient of the policy score function is: $$\begin{aligned}
    \nabla_{\theta}J(\theta) &= \nabla_{\theta}E_{\pi_{\theta}(\tau)}[r(\tau)] \\
    &= \int\nabla_{\theta}\pi_{\theta}(\tau)r(\tau)d\tau = \int\pi_{\theta}(\tau)\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)d\tau \\
    &= E_{\pi_{\theta}(\tau)}[\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)] \\
    &= E_{\pi_{\theta}(\tau)}[(\sum_t\nabla_{\theta}\log\pi_{\theta}(a_t|s_t))(\sum_t r(s_t, a_t))]\end{aligned}$$

**Monte Carlo sampling:** We can’t calculate the expectation exactly
because the distribution is unknown. We use monte carlo sampling to get
N samples and use their average to estimate the expectation:
$$E_{\pi_{\theta}(\tau)}[\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)] \approx \frac{1}{N}\sum_i \nabla_{\theta}\log\pi_{\theta}(\tau^i)r(\tau^i)$$

**Stochastic Policy:** Note that our learned policy would be stochastic
because the environment is stochastic

Variants of Policy Gradients
----------------------------

### Proximal Policy Optimization (PPO) [@PPO]

As a combination of vanilla Policy Gradients [@PG] and Trust Region
Policy Optimization [@TRPO], Proximal Policy Optimization is a more
state of art RL algorithm that is able to reach good performance in
fewer training iterations. It uses an actor-critic framework to restrict
the algorithm so that it is not overly sensitive to large step sizes. By
adding a KL penalty, PPO prevents the new policy from deviating too much
from the old policy. Therefore, PPO can be considered as not only an
advanced version of vanilla policy gradient, but also a method that
stabilizes the movements of the control targets. The details are
outlined in *Algorithm [\[PPO\]]{}*.

**for** iteration=1, 2, . . . **for** actor=1, 2, . . . , N Run policy
$\pi_{\theta_{old}}$ in environment for T time steps Compute advantage
estimates $A^1, ..., A^T$ **end for** Optimize surrogate L wrt $\theta$,
with K epochs and minibatch size $M \leq NT$
$\theta_{old} \leftarrow \theta$ **end for**

### Regularization on Vanilla Policy Gradients and PPO

One of the challenges that training deep neural network faces is the
correlation among each layer’s weights. In order to remove the internal
covariate shift" in deep neural networks, **Batch Normalization**
[@BatchNorm] was proposed to normalize for each training mini batch at
each activation. It is a method that accelerates training of deep neural
networks and acts as a regularizer.

For example, at a particular activation, Batch Normalization will
perform this transformation:

$y_i = \gamma \frac{x_i-\mu_{\beta} }{\sqrt{\sigma_{\beta}^2 + \epsilon}}$

where $$\mu_{\beta} = \frac{1}{m}\sum_{i=1}^mx_i$$
$$\sigma_{\beta}^2 = \frac{1}{m}\sum_{i=1}^m(x_i-\mu_{\beta})^2$$

$y_i$ is the output of the activation, m is the batch size, $x_i$’s are
the activation values of training examples in the mini batch at the
activation, and $\gamma$, $\epsilon$ are trainable variables.

In addition, **L1 and L2 Regularization** are common ways to reduce
overfitting. They both add a regularization term in order to prevent the
coefficients to fit so perfectly to overfit. We implemented these two
methods on neural network for Policy Gradient algorithm.

For L1 regularization, $$\lambda \sum_{i=1}^{k}|w_i|$$ is added to the
original loss, and for L2 regularization,
$$\lambda \sum_{i=1}^{k}w_i^2$$ is added, where there are a total of $k$
weights in all layers of neural network and we don’t penalize for large
bias.

Experiments and Analysis
========================

  --------------------------- ------------ ------------------------- ---------------- -----------------
          Environment          Avg Return   Avg of Best 100 Returns   Solve Criteria   num of episodes
     \[0.5ex\] CartPole-v0       22.53               63.13                 200              3000
   InvertedDoublePendulum-v1     55.12              107.84                 9000             3000
    Discretized CartPole-v0      176.23             200.00                 200              3000
  --------------------------- ------------ ------------------------- ---------------- -----------------

Q-Learning
----------

First, we ran the naive implementation of Q-Learning on both
“CartPole-v0” and “InvertedDouble-Pendulum-v1”. In this version, instead
of applying any optimization, domain knowledge or fine-tuning, we
directly use the observations obtained from OpenAI Gym as the states of
Q-Learning inputs. By default, we use $\epislon = 0.1$ for random
exploration.

For “CartPole-v0", since the action space is discrete, Q-Learning
chooses the action that maximizes the Q-value over all possible actions
at each time step. However, for “InvertedDoublePendulum-v1", since the
action space is continuous, it is not possible to take the max of the
Q-value empirically, therefore, as a compromise, we limit our attention
to 1000 sampled actions over the action space.

The average return of all episodes, the average return of the best 100
episodes, and the solve criteria for each environment are shown in the
first two lines of *Table \[QLearningResult\]*. Not surprisingly, the
result is not very desirable because random exploration of (state,
action) pairs over a large state space is not sufficient to allow the
Q-learning algorithm to learn accurate Q-values for each such pair, even
with a large number of iterations. The performance for
“InvertedDoublePendulum-v1" is especially miserable because we only
optimize over a limited action space.

To solve the above issues, as a second attempt, we try to discretize the
state space for “CartPole-v0". Specifically, we discretize the Cart
Velocity, Pole Velocity At Tip, and Pole Velocity At Tip into 8 bins
each, and we discretize Pole Angle into 10 bins. With discretization,
the size of the new state space is $8^3 \times 10 = 5120$, which is more
manageable for Q-Learning. The result after discretization is shown in
the last line of *Table \[QLearningResult\]*. We are able to achieve the
max return consistently for the the best 100 returns after 3000
episodes.

For “InvertedDoublePendulum-v1", because the action space is continuous,
Q-learning does not seem the right algorithm to go with. Even after
tuning the $\epsilon$ for exploration-exploitation trade off, the
performance does not show any significant improvement. Therefore, we
extensively experiment with Policy Gradients Method on
“InvertedDoublePendulum-v1" in the next section.

Vanilla Policy Gradients
------------------------

![Policy gradient hyperparameter tuning for
InvertedDoublePendulum-v1[]{data-label="prelimPG"}](hyperparameters_tuning.png){width="\columnwidth"}

For policy gradients to work well, we first work on tuning hyper
parameters intensively. The hyper parameters we considered include
number of layers of the neural network, learning rate and size of hidden
layers.

Our approach was to run Policy Gradients with each hyper-parameter
combination for 100 training iterations in order to inspect the average
returns, and 5 times for each combination to investigate on the
variances of average returns. Left image of *Figure \[prelimPG\]* shows
a comparison of results on preliminary hyper-parameter tuning on
“InvertedDoublePendulum-v1" for a small number of training iterations
(100 iterations). Each colored smoothed line shows the improvement of
average return over training iterations on a combination of
hyper-parameters. And the shaded areas around the smoothed lines show
the variances of average returns over 5 random experiments with the
specified hyper-parameters. The format of legends are:
$idp\_\{\$number\_of\_layers\}\_\{\$learning\_rate\}\_\{\$size\}$.

We further tune the best two combinations of hyper parameters for 200
iterations as shown on right side of *Figure \[prelimPG\]*. With 200
iterations, we are already able to achieve promising returns of around
4000, which is much better than naive Q-Learning. We will take number of
layers as 3, learning rate as 1e-3 and the size of each layer 32 as the
baseline for further explorations, because the variance is smaller, and
thus the performance is more stable.

![One-second (9 Hz) simulation of Inverted-double-pendulum-v1 policy
trained by vanilla Policy Gradients
[]{data-label="idp_demo"}](idp_demo.png){width="\columnwidth"}

*Figure \[idp\_demo\]* shows a series of screen shots of the simulation
of Inverted Double Pendulum Problem after training with Policy Gradients
for 500 iterations. It can be observed that the pendulum is actively
adapting to the evolving state-space. The learned policy manages to make
subtle changes in movements from one time frame to another in order to
balance the pendulum on the pole.

Proximal Policy Optimization
----------------------------

![Comparison of Vanilla Policy Gradients and
PPO[]{data-label="vanilla_ppo"}](vanilla_ppo.png){width="255pt"}

There remain several problems to be solved from vanilla version of
Policy Gradients. First, for complex tasks such as Half-Cheetah-v2,
Policy Gradients is too slow to hit a reasonable performance. Second,
Policy Gradients have bad stabilization, the actions being chosen varies
a lot from one state to another, and also from one episode to another.
Therefore, the average return on “InverdDoublePendulum-v1" can reach
9000 at its maximum, but can also fluctuates to less than 2000 very
soon. With the motivation to stabilize, as well as to speed up Vanilla
Policy Gradients, we experiment with Proximal Policy Optimizations on
the same tasks.

*Figure \[vanilla\_ppo\]* shows the comparison between Vanilla Policy
Gradients and PPO on Inverted-Double-Pendulum and Half-Cheetah. For both
environments, PPO significantly outperforms. It manages to converge much
faster than Vanilla Policy Gradients. On Half-Cheetah and with PPO, the
average return spikes in the first 100 iterations and converges to
around 1500 in 500 iterations. However, the average returns of Vanilla
Policy Gradient does not show significant improvement in the first 500
episodes.

Regularization
--------------

![Batch normalization on vanilla policy
gradients[]{data-label="vanilla_bn"}](idp_vanilla_bn.png){width="150pt"}

![Batch normalization on vanilla policy
gradients[]{data-label="vanilla_bn"}](half-cheetah_BN.png){width="150pt"}

We first combine Batch Normalization with vanilla Policy Gradients as a
way of regularization. The returns for both environments are shown in
*Figure \[vanilla\_bn\]*. For ‘InvertedDoublePendulum-v1’, we try batch
normalization coefficients 0.5, 0.8, 0.9 and 0.95. For ‘HalfCheetah-v2’,
we try batch normalization coefficients 0.9, 0.95 and 0.99. Benchmark
represents a vanilla Policy Gradients implementation with pre-tuned
hyper parameters and no further fine tuning. The result is displayed in
*Figure \[vanilla\_bn\]*. Clearly, Batch Normalization outperforms the
Benchmark for both environments, in terms of both stabilizing and
speeding up the training.

![L2 regularization on proximal policy
optimization[]{data-label="ppo_l2"}](idp_ppo_l2.png){width="150pt"}

![L2 regularization on proximal policy
optimization[]{data-label="ppo_l2"}](hc_ppo_l2_accuracy_compare.png){width="150pt"}

Since Proximal Policy Optimization has been shown as a better algorithm
than vanilla Policy Gradients for our tasks, we also apply $L_2$
regularization to Proximal Policy Optimization and train for both
environments. For “InvertedDoublePendulum-v1", we try $L_2$
regularization coefficients $\lambda = 0.0001, 0.001, 0.01, 0.1, 0.5$.
For “HalfCheetah-v2", we try
$\lambda = 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5$. As shown in *Figure
\[ppo\_l2\]*, by using $L_2$ regularization with an appropriate
$\lambda$, we are able to achieve consistently larger average returns
than the benchmark.

![Simulation of HalfCheetah-v2 policy trained by PPO with and wihout
$L_2$
regularization[]{data-label="hc_l2_ppo_case"}](hc_ppo_l2_compare.png){width="200pt"}

For a more intuitive understanding, we rendered the resulting policies
from PPO with and without $L_2$ regularization on environment
‘HalfCheetah-v2’ in *Figure \[hc\_l2\_ppo\_case\]*. Interesting enough,
for the policy trained on PPO and without $L_2$, the cheetah is moving
with an unexpected strategy: it is lying upside down, and moving on its
back. Therefore, it is running slower than the one with $L_2$
regularization, in which the cheetah is running with its legs. One
explanation for this failure case is that without regularization, the
policy function gets stuck in a bad local minimum. $L_2$ regularization,
on the other hand, helps avoid extreme cases by restricting the size of
the weight vectors. Instead of being trapped by bad extremum, it picks
up a solution that is more conservative and ends up in a
closer-to-global minimum solution.

Limitations and Future Directions
=================================

To more comprehensively understand the performance of Reinforcement
algorithms on control tasks, there are many directions future researches
can go for. The key idea would be to extend the the current experiments
to other regularization methods, on other deep reinforcement learning
algorithms, and run for longer episodes.

Other Deep Reinforcement Learning algorithms
--------------------------------------------

Here are several examples of other deep reinforcement learning
algorithms that are worth considering:

### Deep Deterministic Policy Gradient(DDPG)

The DDPG algorithm continuously improve the policy as it explores the
environment. It applies gradient descent to the policy with minibatch
data sampled from a replay pool. It is believed that DDPG can sometimes
find plocies that exceed the performance of the
planner.[@lillicrap2015continuous]

### Reward-Weighted Regression(RWR)

This algorithm formulates the policy optimization as an
Expectation-Maximization problem to avoid the need to manually choose
learning rate. It turns out that the algorithm works well in the
applications of complex high degree-of-freedom
robots.[@peters2007reinforcement]

### Trust Region Policy Optimization(TRPO)

With the introduction of a surrogate loss, this algorithm allows more
precise control on the expected policy improvement. This algorithm is
effective for optimizing large nonlinear policies such as neural
networks.[@schulman2015trust]

Other Regularization Methods
----------------------------

Here are several examples of different regularization methods with their
own pros and cons:

### Dropout

Dropout means randomly ignoring some units in deep neural network for
regularization, which is implemented as setting some randomly chosen
outputs of neurons to zero at each training iteration. Dropout prevents
over-fitting and provides an efficient way of approximately combining
exponentially many different neural network
architectures.[@srivastava2014dropout]

### Layer Normalization

Though Batch Normalization proves to be very useful in many settings, it
has the potential drawback of requiring a big batch size, because a
larger batch better approximates the normalization statistics of the
whole training set. Layer Normalization method is designed to overcome
the limitations of Batch Normalization. We can transpose batch
normalization into layer normalization by computing the mean and
variance used for normalization from all of the summed inputs to the
neurons in a layer on a single training case. It is very effective at
stabilizing the hidden state dynamics in recurrent
networks.[@ba2016layer]

### Weight Normalization

Weight normalization is a reparameterization of the weight vectors in a
neural network that decouples the length of those weight vectors from
their direction. It will potentially improve the conditioning of the
optimization problem and speed up convergence of stochastic gradient
descent. We can view it as a relaxed version of batch normalization,
which simplifies the implementation with the cost of
instability.[@salimans2016weight]

Training for Longer Episodes
----------------------------

We have to admit that because of the limitations in time and computing
resources, we did not run all experiments to convergence. In the future,
running all experiments to convergence or for more iterations (if
convergence is not guaranteed) will help set up a fairer comparison
between the algorithms.
